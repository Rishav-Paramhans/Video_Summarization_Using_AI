Question: Understand how the context size is handled and how come same model has multiple contect size varaints?
Question2: How can we use cpp invocation layer to run the llama- look for a video on YT